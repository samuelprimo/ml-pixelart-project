{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7600561,"sourceType":"datasetVersion","datasetId":4424547}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#CÉLULA 1 (DATASET 89k - 16x16)\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n#Constantes\nIMAGE_SIZE = 16\nIMAGE_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\nDATA_PATH = '../input/pixel-art/sprites.npy'\n\n#Carregar e Pré-processar\nprint(\"Carregando o dataset .npy...\")\nX_train_raw = np.load(DATA_PATH)\n\nprint(f\"Dataset bruto carregado. Shape: {X_train_raw.shape}\")\n\ndef normalize_image(img_array):\n    return (img_array.astype(np.float32) - 127.5) / 127.5\n\nX_train = normalize_image(X_train_raw)\n\n#Verificação de Sanidade\nprint(f\"\\nDataset processado com sucesso.\")\nprint(f\"Formato (Shape): {X_train.shape}\") #Deve ser (89000, 16, 16, 3)\nprint(f\"Valor Mínimo: {X_train.min():.2f}\") #Deve ser -1.0\nprint(f\"Valor Máximo: {X_train.max():.2f}\") #Deve ser 1.0\n\nprint(\"\\nExibindo uma amostra (desnormalizada):\")\nsample_image_to_show = (X_train[0] + 1) / 2.0\nplt.imshow(sample_image_to_show)\nplt.axis('off')\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:16:45.677393Z","iopub.execute_input":"2025-11-12T19:16:45.677729Z","iopub.status.idle":"2025-11-12T19:17:02.797264Z","shell.execute_reply.started":"2025-11-12T19:16:45.677702Z","shell.execute_reply":"2025-11-12T19:17:02.796675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#CÉLULA 2 (WGAN-GP 16x16)\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\n\n#Constantes de Arquitetura\nNOISE_DIM = 100\nBATCH_SIZE = 64\n\n#Gerador (16x16)\ndef build_generator():\n    \"\"\"Converte ruído (100) -> imagem (16x16x3)\"\"\"\n    model = Sequential(name=\"Generator_16x16\")\n    \n    #Semente 4x4\n    model.add(layers.Dense(4 * 4 * 256, input_dim=NOISE_DIM))\n    model.add(layers.LeakyReLU(alpha=0.2))\n    model.add(layers.Reshape((4, 4, 256)))\n    \n    #4x4 -> 8x8\n    model.add(layers.UpSampling2D())\n    model.add(layers.Conv2D(128, kernel_size=3, padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n\n    #8x8 -> 16x16\n    model.add(layers.UpSampling2D())\n    model.add(layers.Conv2D(128, kernel_size=3, padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n\n    #Camada de Saída (16x16x3)\n    model.add(layers.Conv2D(3, kernel_size=3, padding='same', activation='tanh'))\n    \n    print(\"--- Arquitetura do Gerador (16x16) ---\")\n    model.summary()\n    return model\n\n#Crítico (16x16)\ndef build_critic():\n    \"\"\"Converte imagem (16x16x3) -> score (1)\"\"\"\n    model = Sequential(name=\"Critic_16x16\")\n    \n    #16x16 -> 8x8\n    model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding='same', input_shape=IMAGE_SHAPE))\n    model.add(layers.LeakyReLU(alpha=0.2))\n\n    #8x8 -> 4x4\n    model.add(layers.Conv2D(256, kernel_size=3, strides=2, padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n\n    #Camada de Análise (4x4)\n    model.add(layers.Conv2D(512, kernel_size=3, padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n\n    #Saída\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1)) \n    \n    print(\"\\n--- Arquitetura do Crítico (16x16) ---\")\n    model.summary()\n    return model\n\n#Otimizadores\n#Com 89k de dados, o Crítico é \"gênio\". Não precisamos mais do 1e-6.\n#Voltamos para a taxa padrão (1e-4) que \"explodia\" antes.\ncritic_optimizer = Adam(learning_rate=0.0001, beta_1=0.0, beta_2=0.9)\ngenerator_optimizer = Adam(learning_rate=0.0001, beta_1=0.0, beta_2=0.9)\n\n#Criar os Modelos\ngenerator = build_generator()\ncritic = build_critic()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:17:08.557428Z","iopub.execute_input":"2025-11-12T19:17:08.558032Z","iopub.status.idle":"2025-11-12T19:17:08.693771Z","shell.execute_reply.started":"2025-11-12T19:17:08.558008Z","shell.execute_reply":"2025-11-12T19:17:08.692858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#CÉLULA 3 (VERSÃO WGAN-GP LOOP - 16x16)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport tensorflow as tf\n\n#Constantes de Treino\nEPOCHS = 500\nD_STEPS_PER_G_STEP = 5 \nGP_WEIGHT = 10.0   \nSAVE_INTERVAL = 20\n\n#Função de Salvar\ndef save_plot(generator, epoch, noise_dim):\n    \"\"\"\n    Gera uma grade 4x4 de imagens do Gerador e a salva em disco.\n    \"\"\"\n    noise = tf.random.normal([16, noise_dim])\n    generated_images = generator(noise, training=False)\n    #Desnormaliza de [-1, 1] para [0, 1] para visualização\n    generated_images = (generated_images + 1) / 2.0\n    \n    fig, axs = plt.subplots(4, 4, figsize=(8, 8))\n    for i in range(16):\n        axs[i//4, i%4].imshow(generated_images[i])\n        axs[i//4, i%4].axis('off')\n    fig.savefig(f\"/kaggle/working/gan_generated_epoch_{epoch}.png\")\n    plt.close()\n\n#Funções de Perda WGAN-GP\ndef critic_loss(real_output, fake_output):\n    \"\"\"Perda do Crítico: maximizar (real_score - fake_score)\"\"\"\n    real_loss = tf.reduce_mean(real_output)\n    fake_loss = tf.reduce_mean(fake_output)\n    return fake_loss - real_loss\n\ndef generator_loss(fake_output):\n    \"\"\"Perda do Gerador: maximizar (fake_score)\"\"\"\n    return -tf.reduce_mean(fake_output)\n\n#Função da Penalidade de Gradiente (O \"GP\")\ndef gradient_penalty(real_images, fake_images):\n    \"\"\"Calcula a Penalidade de Gradiente (GP) para o WGAN-GP\"\"\"\n    #Amostra aleatória de pontos na linha entre imagens reais e falsas\n    alpha = tf.random.uniform([BATCH_SIZE, 1, 1, 1], 0.0, 1.0)\n    #Cria as imagens interpoladas\n    interpolated = real_images + alpha * (fake_images - real_images)\n\n    with tf.GradientTape() as gp_tape:\n        gp_tape.watch(interpolated)\n        #Obtém a predição (score) do Crítico para essas imagens\n        pred = critic(interpolated, training=True)\n    \n    #Calcula o gradiente do score em relação às imagens interpoladas\n    grads = gp_tape.gradient(pred, [interpolated])[0]\n    #Calcula a norma (magnitude) de cada gradiente\n    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n    #Calcula a penalidade: a média do desvio quadrático da norma de 1\n    gp = tf.reduce_mean((norm - 1.0) ** 2)\n    return gp\n\n#O Loop de Treino Customizado (com tf.function)\n#@tf.function compila a função para performance máxima\n@tf.function\ndef train_step_critic(real_images):\n    \"\"\"Executa um único passo de treino para o Crítico.\"\"\"\n    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n    \n    with tf.GradientTape() as tape:\n        fake_images = generator(noise, training=True)\n        \n        real_output = critic(real_images, training=True)\n        fake_output = critic(fake_images, training=True)\n        \n        c_loss = critic_loss(real_output, fake_output)\n        gp = gradient_penalty(real_images, fake_images)\n        \n        #Perda total = Perda Wasserstein + Penalidade de Gradiente\n        total_critic_loss = c_loss + gp * GP_WEIGHT\n        \n    critic_gradients = tape.gradient(total_critic_loss, critic.trainable_variables)\n    critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n    \n    return total_critic_loss\n\n@tf.function\ndef train_step_generator():\n    \"\"\"Executa um único passo de treino para o Gerador.\"\"\"\n    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n    \n    with tf.GradientTape() as tape:\n        fake_images = generator(noise, training=True)\n        fake_output = critic(fake_images, training=True)\n        g_loss = generator_loss(fake_output)\n        \n    generator_gradients = tape.gradient(g_loss, generator.trainable_variables)\n    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n    \n    return g_loss\n\n#O Loop de Épocas\ndef train_gan(dataset, epochs, d_steps):\n    \"\"\"O loop de treino principal que orquestra as épocas e batches.\"\"\"\n    #Cria o dataset em lotes\n    batched_dataset = tf.data.Dataset.from_tensor_slices(dataset).shuffle(len(dataset)).batch(BATCH_SIZE, drop_remainder=True)\n    \n    print(\"Iniciando o treinamento (WGAN-GP)...\")\n    start_time = time.time()\n    \n    for epoch in range(epochs):\n        epoch_start_time = time.time()\n        c_loss_epoch = []\n        g_loss_epoch = []\n        \n        for i, image_batch in enumerate(batched_dataset):\n            \n            #Sempre treina o Crítico\n            c_loss = train_step_critic(image_batch)\n            c_loss_epoch.append(c_loss)\n            \n            #Treina o Gerador apenas a cada 'd_steps' (5)\n            if (i + 1) % d_steps == 0:\n                g_loss = train_step_generator()\n                g_loss_epoch.append(g_loss)\n                \n        #Fim da Época\n        c_loss_mean = tf.reduce_mean(c_loss_epoch).numpy()\n        g_loss_mean = tf.reduce_mean(g_loss_epoch).numpy() if g_loss_epoch else -1.0\n        \n        epoch_time = time.time() - epoch_start_time\n        print(f\"Época {epoch+1}/{epochs} | D Loss (Crítico): {c_loss_mean:.4f} | G Loss (Gerador): {g_loss_mean:.4f} | Tempo: {epoch_time:.2f}s\")\n        \n        #Salva imagens de amostra em intervalos definidos\n        if (epoch + 1) % SAVE_INTERVAL == 0 or epoch == 0:\n            save_plot(generator, epoch + 1, NOISE_DIM)\n            print(f\" Imagens de exemplo salvas para a época {epoch+1}\")\n            \n    total_time = time.time() - start_time\n    print(f\"\\nTreinamento concluído em {total_time/60:.2f} minutos.\")\n\n#Iniciar o treinamento\ntrain_gan(X_train, EPOCHS, D_STEPS_PER_G_STEP)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:17:12.863518Z","iopub.execute_input":"2025-11-12T19:17:12.864144Z"}},"outputs":[],"execution_count":null}]}